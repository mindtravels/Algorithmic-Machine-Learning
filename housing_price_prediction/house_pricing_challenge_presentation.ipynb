{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# House Pricing Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge is adapted from the well-known '<i>Zillow's Home Value Prediction</i>' competition on Kaggle.\n",
    "In particular, given a dataset containing descriptions of homes on the US property market, your task is to make predictions on the selling price of as-yet unlisted properties. \n",
    "Developing a model which accurately fits the available training data while also generalising to unseen data-points is a multi-faceted challenge that involves a mixture of data exploration, pre-processing, model selection, and performance evaluation.\n",
    "\n",
    "**IMPORTANT**: please refer to the AML course guidelines concerning grading rules. Pay especially attention to the **presentation quality** item, which boils down to: don't dump a zillion of lines of code and plots in this notebook. Produce a concise summary of your findings: this notebook can exist in two versions, a \"scratch\" version that you will use to work and debug, a \"presentation\" version that you will submit. The \"presentation\" notebook should go to the point, and convay the main findings of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview\n",
    "Beyond simply producing a well-performing model for making predictions, in this challenge we would like you to start developing your skills as a machine learning scientist.\n",
    "In this regard, your notebook should be structured in such a way as to explore the following tasks that are expected to be carried out whenever undertaking such a project.\n",
    "The description below each aspect should serve as a guide for your work, but you are strongly encouraged to also explore alternative options and directions. \n",
    "Thinking outside the box will always be rewarded in these challenges.\n",
    "\n",
    "\n",
    "### 1. Data preparation:\n",
    "   \n",
    "_Data exploration_: The first broad component of your work should enable you to familiarise yourselves with the given data, an outline of which is given at the end of this challenge specification. Among others, you can work on:\n",
    "   \n",
    "* Data cleaning, e.g. treatment of categorial variables;\n",
    "* Data visualisation; Computing descriptive statistics, e.g. correlation.\n",
    "\n",
    "_Data Pre-processing_: The previous step should give you a better understanding of which pre-processing is required for the data. This may include:\n",
    "\n",
    "* Normalising and standardising the given data;\n",
    "* Removing outliers;\n",
    "* Carrying out feature selection, possibly using metrics derived from information theory;\n",
    "* Handling missing information in the dataset;\n",
    "* Augmenting the dataset with external information;\n",
    "* Combining existing features.\n",
    "\n",
    "### 2. Model selection\n",
    "An important part of the work involves the selection of a model that can successfully handle the given data and yield sensible predictions. Instead of focusing exclusively on your final chosen model, it is also important to share your thought process in this notebook by additionally describing alternative candidate models. There is a wealth of models to choose from, such as decision trees, random forests, (Bayesian) neural networks, Gaussian processes, LASSO regression, and so on. \n",
    "\n",
    "Irrespective of your choice, it is highly likely that your model will have one or more parameters that require tuning. There are several techniques for carrying out such a procedure, such as cross-validation.\n",
    "\n",
    "### 3. Performance Evaluation\n",
    "The evaluation method for this challenge is the root mean squared error between the logarithm of the predicted value and the logarithm of the observed sales price. Taking logs ensures that errors in predicting expensive houses and cheap houses will have a similar impact on the overall result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files\n",
    "You will be working on two data files, which will be available in ```/mnt/datasets/house/```:\n",
    "\n",
    "* train.csv - The training dataset;\n",
    "* test.csv - The test dataset;\n",
    "* data_description.txt - Full description of each column.\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "A brief outline of the available attributes is given below:\n",
    "\n",
    "* <i>SalePrice</i>: The property's sale price in dollars. This is the target variable that your model is intended to predict;\n",
    "\n",
    "* <i>MSSubClass</i>: The building class;\n",
    "* <i>MSZoning</i>: The general zoning classification;\n",
    "* <i>LotFrontage</i>: Linear feet of street connected to property;\n",
    "* <i>LotArea</i>: Lot size in square feet;\n",
    "* <i>Street</i>: Type of road access;\n",
    "* <i>Alley</i>: Type of alley access;\n",
    "* <i>LotShape</i>: General shape of property;\n",
    "* <i>LandContour</i>: Flatness of the property;\n",
    "* <i>Utilities</i>: Type of utilities available;\n",
    "* <i>LotConfig</i>: Lot configuration;\n",
    "* <i>LandSlope</i>: Slope of property;\n",
    "* <i>Neighborhood</i>: Physical locations within Ames city limits;\n",
    "* <i>Condition1</i>: Proximity to main road or railroad;\n",
    "* <i>Condition2</i>: Proximity to main road or railroad (if a second is present);\n",
    "* <i>BldgType</i>: Type of dwelling;\n",
    "* <i>HouseStyle</i>: Style of dwelling;\n",
    "* <i>OverallQual</i>: Overall material and finish quality;\n",
    "* <i>OverallCond</i>: Overall condition rating;\n",
    "* <i>YearBuilt</i>: Original construction date;\n",
    "* <i>YearRemodAdd</i>: Remodel date;\n",
    "* <i>RoofStyle</i>: Type of roof;\n",
    "* <i>RoofMatl</i>: Roof material;\n",
    "* <i>Exterior1st</i>: Exterior covering on house;\n",
    "* <i>Exterior2nd</i>: Exterior covering on house (if more than one material);\n",
    "* <i>MasVnrType</i>: Masonry veneer type;\n",
    "* <i>MasVnrArea</i>: Masonry veneer area in square feet;\n",
    "* <i>ExterQualv</i>: Exterior material quality;\n",
    "* <i>ExterCond</i>: Present condition of the material on the exterior;\n",
    "* <i>Foundation</i>: Type of foundation;\n",
    "* <i>BsmtQual</i>: Height of the basement;\n",
    "* <i>BsmtCond</i>: General condition of the basement;\n",
    "* <i>BsmtExposure</i>: Walkout or garden level basement walls;\n",
    "* <i>BsmtFinType1</i>: Quality of basement finished area;\n",
    "* <i>BsmtFinSF1</i>: Type 1 finished square feet;\n",
    "* <i>BsmtFinType2</i>: Quality of second finished area (if present);\n",
    "* <i>BsmtFinSF2</i>: Type 2 finished square feet;\n",
    "* <i>BsmtUnfSF</i>: Unfinished square feet of basement area;\n",
    "* <i>TotalBsmtSF</i>: Total square feet of basement area;\n",
    "* <i>Heating</i>: Type of heating;\n",
    "* <i>HeatingQC</i>: Heating quality and condition;\n",
    "* <i>CentralAir</i>: Central air conditioning;\n",
    "* <i>Electrical</i>: Electrical system;\n",
    "* <i>1stFlrSF</i>: First Floor square feet;\n",
    "* <i>2ndFlrSF</i>: Second floor square feet;\n",
    "* <i>LowQualFinSF</i>: Low quality finished square feet (all floors);\n",
    "* <i>GrLivArea</i>: Above grade (ground) living area square feet;\n",
    "* <i>BsmtFullBath</i>: Basement full bathrooms;\n",
    "* <i>BsmtHalfBath</i>: Basement half bathrooms;\n",
    "* <i>FullBath</i>: Full bathrooms above grade;\n",
    "* <i>HalfBath</i>: Half baths above grade;\n",
    "* <i>BedroomAbvGr</i>: Number of bedrooms above basement level;\n",
    "* <i>KitchenAbvGr</i>: Number of kitchens;\n",
    "* <i>KitchenQual</i>: Kitchen quality;\n",
    "* <i>TotRmsAbvGrd</i>: Total rooms above grade (does not include bathrooms);\n",
    "* <i>Functional</i>: Home functionality rating;\n",
    "* <i>Fireplaces</i>: Number of fireplaces;\n",
    "* <i>FireplaceQu</i>: Fireplace quality;\n",
    "* <i>GarageType</i>: Garage location;\n",
    "* <i>GarageYrBlt</i>: Year garage was built;\n",
    "* <i>GarageFinish</i>: Interior finish of the garage;\n",
    "* <i>GarageCars</i>: Size of garage in car capacity;\n",
    "* <i>GarageArea</i>: Size of garage in square feet;\n",
    "* <i>GarageQual</i>: Garage quality;\n",
    "* <i>GarageCond</i>: Garage condition;\n",
    "* <i>PavedDrive</i>: Paved driveway;\n",
    "* <i>WoodDeckSF</i>: Wood deck area in square feet;\n",
    "* <i>OpenPorchSF</i>: Open porch area in square feet;\n",
    "* <i>EnclosedPorch</i>: Enclosed porch area in square feet;\n",
    "* <i>3SsnPorch</i>: Three season porch area in square feet;\n",
    "* <i>ScreenPorch</i>: Screen porch area in square feet;\n",
    "* <i>PoolArea</i>: Pool area in square feet;\n",
    "* <i>PoolQC</i>: Pool quality;\n",
    "* <i>Fence</i>: Fence quality;\n",
    "* <i>MiscFeature</i>: Miscellaneous feature not covered in other categories;\n",
    "* <i>MiscVal</i>: Value (in dollars) of miscellaneous feature;\n",
    "* <i>MoSold</i>: Month sold;\n",
    "* <i>YrSold</i>: Year sold;\n",
    "* <i>SaleType</i>: Type of sale;\n",
    "* <i>SaleCondition</i>: Condition of sale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset\n",
    "First, We read dataset and explore how many attributes, samples, type of attribute and show some samples.\n",
    "\n",
    "There are 1200 samples of training set and 81 columns where the last column 'SalePrice' which is our target we want to predict and the others are attributes.\n",
    "\n",
    "There are many missing value in some attributes such as 'Alley', 'PoolQC', 'Fence' and 'MiscFeature'.\n",
    "\n",
    "The first colums 'Id' is just the numerical order of sample which is useless for our model, so we will remove it in the feature selection step.\n",
    "\n",
    "Then we split attributes into numerical-attributes and categorical-attributes that we can do following steps conveniently. We have 36 numerical-attributes and 43 categorical-attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove attributes which has a lot of missing values\n",
    "First, we see the number of missing values per attribute and decide to remove attributes which has the number of missing value greater than 25% datapoints. Keeping these attributes can affect negatively to our performance because we have to treat these missing values (typicaly by mean or zero with numerical attributes and 'None' with categorical attributes.\n",
    "\n",
    "5 attributes are removed at this step: ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce skewless of target variable\n",
    "After draw distribution of target variable ('SalePrice'), we relize that target has skewed right distribution and it's mean is big (180000). Thus, we decide to get logarithm of target as new target. It not only reduce the skewless of target, but also reduce the huge errors caused by the most expensive houses that can result in negative effects on the overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop numerical attributes which have less correlation with target\n",
    "We use correlation to examine which numerical attributes are removed. We set a threshold equal 0.02 that is the correlation value of 'Id' attribute with target. We know that the 'Id' attribute dont have any relation with target, so we decide set it as a threshold. Therefore, all numerical attributes, whic have correlation value with target less than 0.02, are removed.\n",
    "\n",
    "3 numerical attributes are removed at this step: ['Id', 'BsmtHalfBath', 'BsmtFinSF2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop categorical attributes which have less association with target by using mutual information\n",
    "To keep the important categorical attribute and remove the useless one, we use the matual information of categorical attribute with the target. The threshold is set at 0.01.\n",
    "\n",
    "7 categorical attributes are removed at this step: ['Street', 'Utilities', 'LotConfig', 'LandSlope', 'Condition2', 'RoofMatl', 'Heating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove one among two numerical attributes which has high correlation\n",
    "We find pairs of numerical attributes which have correlation value greater than 0.8 and define them as high correlation. And with a high correlated pair, we should remove one of them. This is particularly good for linear models\n",
    "\n",
    "4 numerical attributes are removed at this step: ['GarageYrBlt', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat missing values\n",
    "Now, we treat missing values:\n",
    "\n",
    "In term of categorical attributes. We will replace these missing value as 'None' like that the house dont have these features.\n",
    "\n",
    "In term of numerical-attributes, there are 2 attributes which have missing values:\n",
    "\n",
    "+ LotFrontage: Linear feet of street connected to property\n",
    "+ MasVnrArea: Masonry veneer area in square feet. We will replace the missing values of these attributes as its min value which mean that the house dont have street connected to property and Masonry veneer area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers\n",
    "\n",
    "To findout outliers, we show scatter plots for target versus numerical attributes as we can see in the scratch version.\n",
    "\n",
    "It is a handcraft step that we will see each plot to define which points are outliers. Then we set the condition to remove these outliers\n",
    "\n",
    "+ LotFrontage: We see one outlier which has LotFrontage value > 300\n",
    "+ LotArea: We see four outliers which have LotArea value > 100000\n",
    "+ GrLivArea: We see one outlier which has GrLivArea value > 4000 and SalePrice < 300000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical attributes into numberical attribute\n",
    "To make model work, we have to convert categorical attributes into numerical attributes. We avoid using Onehot encoding technique (get_dumies) for all categorical distributes because there are some categorical attributes which are ordinal. For example, 'ExterQual' attribute\n",
    "\n",
    "ExterQual: Evaluates the quality of the material on the exterior\n",
    "\n",
    "+ Ex: Excellent\n",
    "+ Gd: Good\n",
    "+ TA: Average/Typical\n",
    "+ Fa: Fair\n",
    "+ Po: Poor\n",
    "\n",
    "With these ordinal categorical attributes, we assign each categorical value with a number by order. For example:\n",
    "\n",
    "ExterQual: Evaluates the quality of the material on the exterior\n",
    "\n",
    "+ Ex: Excellent = 5\n",
    "+ Gd: Good = 4\n",
    "+ TA: Average/Typical = 3\n",
    "+ None: None = 2\n",
    "+ Fa: Fair = 1\n",
    "+ Po: Poor = 0\n",
    "\n",
    "The 'None' categorical value is assigned to median value\n",
    "\n",
    "With non-ordinal categorical attributes, we use Onehot encoding technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising the data\n",
    "\n",
    "Now, the data is quite clean, we will standardising the data because models work well with small values.\n",
    "\n",
    "In addition standardizing the features around the center with a standard deviation of 1 is important when we compare measurements that have different units. Variables that are measured at different scales do not contribute equally to the analysis and might end up creating a bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train set and test set\n",
    "\n",
    "Train data will be split into train set and validated set. That we will use to evaluate and select model. Test data will be evaluate for our chosen model as the final performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Model selection\n",
    "Firstly, we can see a linear correlation after researching the correlation between the target values (housing prices) and features. For example, the price of house tend to increase corresponding to the rising of some following features : Total square feet of Living area, Rates the overall material and finish of the house, Total square feet of basement area, etc.. This influences us to try several linear regression model first. They are Lasso, Lar-lasso, Ridge Regression, ElasticNet, Polynomial Regression. These linear regression model may be well-fitted to numerical features, howerver, in term of categorical features that do not have linear correlation with the target value, we will try some non-linear classifiers such as RandomForest, MLPRegression to see their performances on our dataset, then compare the overal performance of both linear and non-linear classifier in our dataset, from that we can choose the most suitable classifier for our problem. To evaluate the performance of each classifier, we use two types of metrics expressed below, namely Root Mean Square Error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Metrics used\n",
    "In regression model for supervised learning, we use 2 types of matrix to evaluate the regression classifiers, namely Root-mean-square error and R-squared. \n",
    "\n",
    "### Root Mean Square Error (RMSE):\n",
    "\n",
    "The root-mean-square error (RMSE) is a frequently used to measure the differences between values (sample or population values) predicted by a model or an estimator and the values observed. \n",
    "\n",
    "\\begin{equation*}\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big({y_i -f_i}\\Big)^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "### R-squared: \n",
    "R-squared ($R_2$) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. So, if the $R_2$ of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs. \n",
    "\n",
    "\\begin{equation*}\n",
    "R_2 = 1 - \\frac{SSres}{SStot}\n",
    "\\end{equation*}\n",
    "\n",
    "+ $SS_{res}$ is proportional to the unexplained variation\n",
    "\\begin{equation*}\n",
    "SS_{res}= \\Sigma_{i=1}^{n}{\\Big({y_i-f_i}\\Big)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "+ $SS_{tot}$ is proportional to total variation of the observed data \n",
    "\\begin{equation*}\n",
    "SS_{tot} = \\Sigma_{i=1}^{n}{\\Big({y_i-y_{mean}}\\Big)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "+ $y_{i}$ is the i-th observed data in a N-point-dataset marked y1, y2, ..., yn\n",
    "+ $f_{i}$ is the predicted value corresponding to yi, produced by a regression model\n",
    "+ $y_{mean}$ is the mean of the observed data: \n",
    "\\begin{equation*}\n",
    "y_{mean} = \\frac{1}{n}{\\Sigma_{i=1}^{n}{\\Big({y_i}\\Big)}}\n",
    "\\end{equation*}\n",
    "\n",
    "In the best case, the modeled values exactly match the observed values, which results in $SS_{res}$ = 0 and $R_2$ = 1. A baseline model, which always predicts $y_{mean}$, will have $R_2$ = 0. Models that have worse predictions than this baseline will have a negative $R_2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did tuning process to find the best hyperparameter for each classifier. Due to this tuning process, we may get objective results of these classifiers. Observing these results returned from evaluation metrics, we can arrange the performances from best downto worst: Lasso, Lasso-Lars, Ridge Regression, Random Forest, Polynomial Regression (degress=2), ElasticNet. It's likely as we expect, linear classifiers seems to be better than non-linear classifers. The best result regarding to both RMSE and $R_2$ is recorded from Lasso, with RMSE = 0.11231375810425286 and $R_2$= 0.9201704361210433. \n",
    "\n",
    "ElasticNet has the worst performance, so we can neglect it. Polynomial Regression(degree 2) also do not have the good performance; in addition, it takes a long time to transform the input data into the higher-dimensional space, this can not only easily cause overfitting but also be inefficient for computational process. Thus, we can also neglect it. Random Forest has the worse performance recorded from $R_2$, compared to Lasso, Lasso-Lars and Ridge Regression, so we will not use it. \n",
    "\n",
    "Rigge Regression tried to minimize the loss function with the constraint of $L_2$ -normalization for coefficients. The larger the value of alpha, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. In our problem, we are trying to find features most correlated with the target to enhance their coefficient. Thus, the consequence of the shrinkage may affect to coefficients that do not have much meaning to the target value. That's why we will not choose Rigde Regression classifier. \n",
    "\n",
    "Finally, our option for the most suitalbe classifier is Lasso, which tries to mininize the loss function based on the contraints of L1-normalization for coefficients, m has the best result from observation, and have a well represent of the correlation between important features and target values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance evaluation\n",
    "After having the best models for each method, we use cross validation technique to evaluate between models upon two matrix defined above.\n",
    "\n",
    "By doing cross-validation, we can use all data both for training while evaluating our algorithm on examples it has never seen before. It is very usefull for this small dataset (1200 sample in training set)\n",
    "\n",
    "In addition, with cross-validation, we can examine our algorithm is consistent and we can be confident with the performance\n",
    "\n",
    "|---|   Linear polynomial  |\tRidge |\tLasso  |\tLarLasso |\tElastic |\tRandomForest |\n",
    "|---|---|---|---|---|---|---|\n",
    "|RMSE     |   0.2115 \t         | 0.1203 |\t0.1123 |\t0.1124 \t |   0.3979 |\t0.1366       | \n",
    "|R2       |       0.7172 \t     | 0.9083 | 0.9201 |\t0.9199 \t |  -0.0019 |\t0.8818       | \n",
    "\n",
    "Finally, we choose Lasso model to test the performance in test dataset:\n",
    "\n",
    "+ Lasso RMSE test: 0.19284496052755742\n",
    "+ Lasso R2 test: 0.7423606420452732\n",
    "\n",
    "The performance of Lasso model in test set is lower than train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Serendipity\n",
    "\n",
    "+ We used mutual information to find the important categorical attributes\n",
    "+ We use R-squared metric to evaluate the performance of classifier\n",
    "+ We avoid use Onehot encoding for all categorical attributes. Instead of that, we find ordinal categorical attributes and label manually.\n",
    "+ We tuning parameter to find the best model for each algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
